{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/AWS-Bedrock-LLMs-in-Google-Colab/blob/master/bedrock_nova_colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc90908",
      "metadata": {
        "id": "2fc90908"
      },
      "source": [
        "# Amazon Bedrock ‚Äì Simple Inference (Nova Lite) in Google Colab\n",
        "\n",
        "This notebook performs a minimal text inference against **`amazon.nova-lite-v1:0`** on Amazon Bedrock using credentials stored in **Colab `userdata`**.\n",
        "\n",
        "> **Before you run this notebook**  \n",
        "> Go to: **Colab ‚Üí Tools ‚Üí User data** and add the following keys:\n",
        ">\n",
        "> - `AWS_ACCESS_KEY_ID`  \n",
        "> - `AWS_SECRET_ACCESS_KEY`  \n",
        "> - `AWS_DEFAULT_REGION` (e.g., `us-east-1`)  \n",
        "> - *(optional)* `AWS_BEARER_TOKEN_BEDROCK` ‚Äì if this value is actually an AWS **session token**, it will be mapped to `AWS_SESSION_TOKEN` automatically below.\n",
        ">\n",
        "> If you are using long‚Äëlived IAM user credentials, you typically **won't** need a session token. If you are using temporary creds (STS), you **will** need it.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. INSTALL LIBRARIES\n",
        "# ==============================================================================\n",
        "# We need the AWS SDK for Python (boto3) to communicate with Bedrock.\n",
        "\n",
        "import sys, subprocess, importlib\n",
        "def pip_install(pkg):\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "for pkg in [\"boto3\", \"botocore\"]:\n",
        "    pip_install(pkg)\n",
        "\n",
        "print(\"‚úî Dependencies ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-pNhl6lSWJu",
        "outputId": "e62f1f82-a386-4860-b02d-8dec6e239dc0"
      },
      "id": "H-pNhl6lSWJu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úî Dependencies ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. IMPORTS AND CONFIGURATION\n",
        "# ==============================================================================\n",
        "import boto3\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"‚úÖ Libraries imported.\")\n",
        "\n",
        "# Load credentials securely from Colab's userdata secrets manager.\n",
        "try:\n",
        "    aws_access_key_id = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    aws_secret_access_key = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "    aws_default_region = userdata.get('AWS_DEFAULT_REGION')\n",
        "    print(\"‚úÖ AWS credentials loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"üõë Error loading credentials. Please ensure you have set them up in Colab secrets. Error: {e}\")\n",
        "    # Stop execution if credentials are not found\n",
        "    raise SystemExit()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. SETUP BEDROCK CLIENT\n",
        "# ==============================================================================\n",
        "# Create a boto3 client to interact with the Bedrock Runtime.\n",
        "# This client will handle the API requests to the model.\n",
        "bedrock_runtime = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name=aws_default_region,\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key\n",
        ")\n",
        "print(f\"‚úÖ Boto3 client created for region: {aws_default_region}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEHvvPQwSZRk",
        "outputId": "a49b35b6-389a-4fc2-f558-f0479ba16233"
      },
      "id": "WEHvvPQwSZRk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported.\n",
            "‚úÖ AWS credentials loaded successfully.\n",
            "‚úÖ Boto3 client created for region: eu-north-1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. PREPARE THE INFERENCE PAYLOAD\n",
        "# ==============================================================================\n",
        "# Define the model ID and the prompt for the model.\n",
        "model_id = \"amazon.nova-lite-v1:0\"\n",
        "user_prompt = \"Explain the difference between a star and a planet in three simple points.\"\n",
        "\n",
        "# Structure the payload according to the model's expected input format.\n",
        "# The `body` contains the inference configuration and the user message.\n",
        "inference_payload = {\n",
        "    \"inferenceConfig\": {\n",
        "        \"max_new_tokens\": 1000,\n",
        "        \"temperature\": 0.7, # Controls randomness. Lower is more predictable.\n",
        "        \"top_p\": 0.9        # Nucleus sampling.\n",
        "    },\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"text\": user_prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert the Python dictionary to a JSON string.\n",
        "body = json.dumps(inference_payload)\n",
        "print(\"‚úÖ Inference payload prepared.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. INVOKE THE MODEL AND PROCESS THE RESPONSE\n",
        "# ==============================================================================\n",
        "try:\n",
        "    print(\"\\nüöÄ Sending request to Bedrock...\")\n",
        "    # Invoke the model with the prepared payload.\n",
        "    response = bedrock_runtime.invoke_model(\n",
        "        modelId=model_id,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        body=body\n",
        "    )\n",
        "    print(response)\n",
        "\n",
        "    # The response body is a streaming object, so we need to read and parse it.\n",
        "    response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "    # ‚ùó CORRECTED PART ‚ùó\n",
        "    # Extract the generated text from the model's response.\n",
        "    # The structure for this model should be {'output': {'message': {'content': [{'text': '...'}]}}, ...}\n",
        "    model_response = response_body['output']['message']['content'][0]['text']\n",
        "\n",
        "\n",
        "    # --- Display the results ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ INFERENCE COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\\nüó£Ô∏è USER PROMPT:\\n{user_prompt}\")\n",
        "    print(\"\\nü§ñ MODEL RESPONSE:\")\n",
        "    print(model_response)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"üõë An error occurred during inference: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By9nbq5kSAYN",
        "outputId": "f0d76f83-1229-4241-a388-9278cd9d7e05"
      },
      "id": "By9nbq5kSAYN",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Inference payload prepared.\n",
            "\n",
            "üöÄ Sending request to Bedrock...\n",
            "{'ResponseMetadata': {'RequestId': '2ba3e8e4-0347-4dd8-a176-45f69e43918c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 03 Oct 2025 13:37:08 GMT', 'content-type': 'application/json', 'content-length': '1240', 'connection': 'keep-alive', 'x-amzn-requestid': '2ba3e8e4-0347-4dd8-a176-45f69e43918c', 'x-amzn-bedrock-invocation-latency': '1135', 'x-amzn-bedrock-cache-write-input-token-count': '0', 'x-amzn-bedrock-cache-read-input-token-count': '0', 'x-amzn-bedrock-output-token-count': '221', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': <botocore.response.StreamingBody object at 0x791cc2c8c580>}\n",
            "\n",
            "==================================================\n",
            "‚úÖ INFERENCE COMPLETE\n",
            "==================================================\n",
            "\n",
            "üó£Ô∏è USER PROMPT:\n",
            "Explain the difference between a star and a planet in three simple points.\n",
            "\n",
            "ü§ñ MODEL RESPONSE:\n",
            "Certainly! Here are three simple points to explain the difference between a star and a planet:\n",
            "\n",
            "1. **Energy Source**:\n",
            "   - **Star**: A star generates its own light and heat through nuclear fusion, where hydrogen atoms fuse to form helium in its core, releasing vast amounts of energy.\n",
            "   - **Planet**: A planet does not generate its own light and heat. Instead, it reflects light from a star, like the Sun.\n",
            "\n",
            "2. **Composition**:\n",
            "   - **Star**: Stars are primarily composed of hydrogen and helium, with trace amounts of heavier elements.\n",
            "   - **Planet**: Planets can be made up of a variety of materials, including gases (like gas giants), rocks (like terrestrial planets), ice, and metals.\n",
            "\n",
            "3. **Orbital Path**:\n",
            "   - **Star**: Stars typically remain fixed in their positions in the sky relative to each other, although they do move slowly over long periods.\n",
            "   - **Planet**: Planets orbit around stars, such as the Sun in our solar system. Their positions relative to the stars change over time due to their orbits.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}